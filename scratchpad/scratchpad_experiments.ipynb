{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratchpad_radcad import model, simulation, experiment\n",
    "from radcad import Experiment, Engine\n",
    "import pandas as pd\n",
    "import openai\n",
    "# This model is taken from Danilo's (@danlessa) radcad example model. \n",
    "# All thanks to CadLabs and the contributors to radcad and cadcad.\n",
    "# https://github.com/CADLabs/radCAD/tree/master/examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the openaikey from .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_key = os.getenv(\"OPENAI_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cadcad_gpt import CadCAD_GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cadcad_gpt = CadCAD_GPT(openai_key, model, simulation, experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_B_test(par1, val1, par2, val2, natural_language_request):\n",
    "    \"\"\"\n",
    "    Perform an A/B test by updating the parameters par1 and par2 with values val1 and val2 respectively.\n",
    "    Runs the experiment and returns the result of the A/B test.\n",
    "\n",
    "    Parameters:\n",
    "    par1 (str): The name of the first parameter to update.\n",
    "    val1 (float): The value to set for the first parameter.\n",
    "    par2 (str): The name of the second parameter to update.\n",
    "    val2 (float): The value to set for the second parameter.\n",
    "    natural_language_request (str): The natural language request that tests a metric on the dataframe generated by the A/B test. eg: which subset has the max value of prey_population?\n",
    "\n",
    "    Returns:\n",
    "    str: The code to run for further analysis.\n",
    "    \"\"\"\n",
    "    initial_par1_value = simulation.model.params[par1][0]\n",
    "    initial_par2_value = simulation.model.params[par2][0]\n",
    "    \n",
    "    simulation.model.params.update({\n",
    "        par1: [val1, initial_par1_value],\n",
    "        par2: [initial_par2_value, val2]\n",
    "        })\n",
    "    \n",
    "    experiment = Experiment(simulation)\n",
    "    experiment.engine = Engine()\n",
    "    result = experiment.run()\n",
    "    # Convert the results to a pandas DataFrame\n",
    "    df = pd.DataFrame(result)\n",
    "\n",
    "    systemprompt = f'''You are a python pandas helper. You are working with a dataframe called df and it has the following columns: {model.state.keys()}. The subset column 0 and 1 are to be compared on the metric that is given by the user. The code must print a string at the end explaining the comparision. Only output the python output enclosed in ``` backticks.'''\n",
    "    completion = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            max_tokens=500,\n",
    "            messages=[\n",
    "                {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"{systemprompt}\"\n",
    "                },\n",
    "                {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{natural_language_request}\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            top_p=0.5,\n",
    "        )\n",
    "    answer = completion.choices[0].message.content\n",
    "    code_to_run = answer.split('```')[1].replace('python', '')\n",
    "    print(code_to_run)\n",
    "    if 'df' in code_to_run:\n",
    "        exec(code_to_run)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Group by 'subset' and calculate the max 'prey_population' for each subset\n",
      "max_prey_population = df.groupby('subset')['prey_population'].max()\n",
      "\n",
      "# Find the subset with the maximum 'prey_population'\n",
      "max_subset = max_prey_population.idxmax()\n",
      "max_value = max_prey_population.max()\n",
      "\n",
      "# Print the result\n",
      "print(f\"Subset {max_subset} has the maximum value of prey_population, which is {max_value}.\")\n",
      "\n",
      "Subset 1 has the maximum value of prey_population, which is 105.0.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(A_B_test('prey_reproduction_rate', 0.1, 'predator_interaction_factor', 0.2, 'which subset has the max value of prey_population?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_sweep(par1, range, natural_language_request):\n",
    "    \"\"\"\n",
    "    Perform a parameter sweep by updating the parameter par1 with values in range.\n",
    "    Runs the experiment and returns the result of the parameter sweep.\n",
    "\n",
    "    Parameters:\n",
    "    par1 (str): The name of the parameter to update.\n",
    "    range (list): The list of values to set for the parameter.\n",
    "    natural_language_request (str): The natural language request that tests a metric on the dataframe generated by the parameter sweep. eg: which subset has the max value of prey_population?\n",
    "\n",
    "    Returns:\n",
    "    str: The code to run for further analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    simulation.model.params.update({\n",
    "        par1: range\n",
    "        })\n",
    "    \n",
    "    experiment = Experiment(simulation)\n",
    "    experiment.engine = Engine()\n",
    "    result = experiment.run()\n",
    "    # Convert the results to a pandas DataFrame\n",
    "    df = pd.DataFrame(result)\n",
    "\n",
    "    systemprompt = f'''You are a python pandas helper. \n",
    "    You are working with a dataframe called df and it has the following columns: {model.state.keys()}. \n",
    "    The subset column values represent different experiments like 0,1,2.. and they are to be compared on the metric that is given by the user.\n",
    "    The code must print the intermediate variables and as well as a string at the end explaining the comparision. Only output the python output enclosed in ``` backticks.'''\n",
    "    completion = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            max_tokens=500,\n",
    "            messages=[\n",
    "                {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"{systemprompt}\"\n",
    "                },\n",
    "                {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{natural_language_request}\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            top_p=0.5,\n",
    "        )\n",
    "    answer = completion.choices[0].message.content\n",
    "    code_to_run = answer.split('```')[1].replace('python', '')\n",
    "    print(code_to_run)\n",
    "    if 'df' in code_to_run:\n",
    "        exec(code_to_run)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Group by 'subset' and find the minimum 'prey_population' for each subset\n",
      "min_prey_population_per_subset = df.groupby('subset')['prey_population'].min()\n",
      "\n",
      "# Print the intermediate variable\n",
      "print(\"Minimum prey population per subset:\")\n",
      "print(min_prey_population_per_subset)\n",
      "\n",
      "# Find subsets where the prey population does not go extinct (min prey population > 0)\n",
      "non_extinct_subsets = min_prey_population_per_subset[min_prey_population_per_subset > 0].index.tolist()\n",
      "\n",
      "# Print the subsets where the prey population does not go extinct\n",
      "print(\"Subsets where the prey population does not go extinct:\")\n",
      "print(non_extinct_subsets)\n",
      "\n",
      "# Explanation string\n",
      "explanation = f\"The prey population does not go extinct in the following subsets: {non_extinct_subsets}\"\n",
      "print(explanation)\n",
      "\n",
      "Minimum prey population per subset:\n",
      "subset\n",
      "0    45.0\n",
      "1     0.0\n",
      "2     0.0\n",
      "3     0.0\n",
      "4     0.0\n",
      "5     0.0\n",
      "6     0.0\n",
      "Name: prey_population, dtype: float64\n",
      "Subsets where the prey population does not go extinct:\n",
      "[0]\n",
      "The prey population does not go extinct in the following subsets: [0]\n"
     ]
    }
   ],
   "source": [
    "param_sweep('prey_reproduction_rate', [0, 0.1, 0.2, 0.3, 0.4, 0.5, 5], 'In which subsets does the prey_population not go extinct?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range = [i*0.1 for i in range(30)]\n",
    "\n",
    "# simulation.model.params.update({\n",
    "#     'prey_reproduction_rate': [0.1,0.2,0.3,0.4,0.5,0.6]\n",
    "#     })\n",
    "\n",
    "# experiment = Experiment(simulation)\n",
    "# experiment.engine = Engine()\n",
    "# result = experiment.run()\n",
    "# # Convert the results to a pandas DataFrame\n",
    "# df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Group by 'subset' and calculate the mean of 'prey_population'\n",
      "subset_means = df.groupby('subset')['prey_population'].mean()\n",
      "\n",
      "# Filter the groups with a mean less than 200\n",
      "subsets_with_low_mean = subset_means[subset_means < 200].index.tolist()\n",
      "\n",
      "# Print the result\n",
      "print(f\"Subsets with a mean prey_population of less than 200: {subsets_with_low_mean}\")\n",
      "\n",
      "Subsets with a mean prey_population of less than 200: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "param_sweep('prey_reproduction_rate', 'range', 'In which subsets does the prey_population have a mean value of less than 200?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range = [i*0.1 for i in range(30)]\n",
    "\n",
    "simulation.model.params.update({\n",
    "    'prey_reproduction_rate': [0.1,0.2,0.3,0.4,0.5,0.6]\n",
    "    })\n",
    "\n",
    "experiment = Experiment(simulation)\n",
    "experiment.engine = Engine()\n",
    "result = experiment.run()\n",
    "# Convert the results to a pandas DataFrame\n",
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsets with a mean prey_population of less than 200: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "subset_means = df.groupby('subset')['prey_population'].mean()\n",
    "\n",
    "# Filter the groups with a mean less than 200\n",
    "subsets_with_low_mean = subset_means[subset_means < 200].index.tolist()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Subsets with a mean prey_population of less than 200: {subsets_with_low_mean}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subset\n",
       "0      80.190809\n",
       "1      99.000999\n",
       "2    3046.802198\n",
       "3    4066.255744\n",
       "4    4327.371628\n",
       "5    4461.342657\n",
       "Name: prey_population, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mPlanner Agent:\u001b[0m\n",
      "I have made a plan to follow:\n",
      "Step 1 use change_param to change the prey reproduction rate to 0.1\n",
      "Step 2 use analysis_agent to calculate the mean of the prey_population column\n",
      "\n",
      "\n",
      "\u001b[36mExecutor Agent:\u001b[0m\n",
      "Thought: My task is to use change_param to change the prey reproduction rate to 0.1\n",
      "Action: I should call change_param function with these {'param': 'prey_reproduction_rate', 'value': 0.1} arguments.\n",
      "Observation: \n",
      "\u001b[32mnew prey_reproduction_rate value is 0.1 and the simulation dataframe is updated\u001b[0m\n",
      "\u001b[36mExecutor Agent:\u001b[0m\n",
      "Thought: My task is to use analysis_agent to calculate the mean of the prey_population column\n",
      "Action: I should call analysis_agent function with these {'question': 'What is the mean of the prey_population column?'} arguments.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"df['prey_population'].mean()\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m80.19080919080919\u001b[0m\u001b[32;1m\u001b[1;3mThe mean of the `prey_population` column is approximately 80.19.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Observation: \n",
      "\u001b[33mThe mean of the `prey_population` column is approximately 80.19.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cadcad_gpt('can you change the prey reproduction rate to 0.1 and tell me the mean of the prey_population column?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mPlanner Agent:\u001b[0m\n",
      "I have made a plan to follow:\n",
      "Step 1 use function change_param to change the prey reproduction rate to 0.2\n",
      "Step 2 use function analysis_agent to get the mean of the prey_population column.\n",
      "\n",
      "\n",
      "\u001b[36mExecutor Agent:\u001b[0m\n",
      "Thought: My task is to use function change_param to change the prey reproduction rate to 0.2\n",
      "Action: I should call change_param function with these {'param': 'prey_reproduction_rate', 'value': 0.2} arguments.\n",
      "Observation: \n",
      "\u001b[32mnew prey_reproduction_rate value is 0.2 and the simulation dataframe is updated\u001b[0m\n",
      "\u001b[36mExecutor Agent:\u001b[0m\n",
      "Thought: My task is to use function analysis_agent to get the mean of the prey_population column.\n",
      "Action: I should call analysis_agent function with these {'question': 'What is the mean of the prey_population column?'} arguments.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"df['prey_population'].mean()\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m99.000999000999\u001b[0m\u001b[32;1m\u001b[1;3mThe mean of the `prey_population` column is approximately 99.001.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Observation: \n",
      "\u001b[33mThe mean of the `prey_population` column is approximately 99.001.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cadcad_gpt('can you change the prey reproduction rate to 0.2 and tell me the mean of the prey_population column?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TE_demo_cadcadGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
